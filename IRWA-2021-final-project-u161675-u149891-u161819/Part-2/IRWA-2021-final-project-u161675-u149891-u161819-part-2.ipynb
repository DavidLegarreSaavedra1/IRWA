{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1) Tweets pre-processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This dataset consists only of one line where all the 2399 tweets by the WHO twitter account. \n",
    "We use the json python library to load this text as a dictionary since the dataset is stored \n",
    "in JSON format. \n",
    "\"\"\"\n",
    "docs_path = 'dataset_tweets_WHO.txt'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readline()\n",
    "tweets = json.loads(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First tweet structure:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Wed Oct 13 09:15:58 +0000 2021',\n",
       " 'id': 1448215930178310144,\n",
       " 'id_str': '1448215930178310144',\n",
       " 'full_text': \"It's International Day for Disaster Risk Reduction\\n\\n#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\\n\\nStart learning today &amp; be #Ready4Response:\\nðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\",\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 274],\n",
       " 'entities': {'hashtags': [{'text': 'OpenWHO', 'indices': [52, 60]},\n",
       "   {'text': 'Ready4Response', 'indices': [232, 247]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/hBFFOF0xKL',\n",
       "    'expanded_url': 'https://bit.ly/3wCa0Dr',\n",
       "    'display_url': 'bit.ly/3wCa0Dr',\n",
       "    'indices': [251, 274]}],\n",
       "  'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 1448215398814560259,\n",
       "    'id_str': '1448215398814560259',\n",
       "    'indices': [275, 298],\n",
       "    'media_url': 'http://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/ext_tw_video_thumb/1448215398814560259/pu/img/0COksfUiCnS3MuNy.jpg',\n",
       "    'url': 'https://t.co/fgZY22RWuS',\n",
       "    'display_url': 'pic.twitter.com/fgZY22RWuS',\n",
       "    'expanded_url': 'https://twitter.com/WHO/status/1448215930178310144/video/1',\n",
       "    'type': 'video',\n",
       "    'sizes': {'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1920, 'h': 1080, 'resize': 'fit'},\n",
       "     'medium': {'w': 1200, 'h': 675, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 383, 'resize': 'fit'}},\n",
       "    'video_info': {'aspect_ratio': [16, 9],\n",
       "     'duration_millis': 97639,\n",
       "     'variants': [{'bitrate': 256000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/480x270/izK3M-OCh-xYweXi.mp4?tag=12'},\n",
       "      {'bitrate': 832000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/640x360/deOwD7OuDaJ7uiHk.mp4?tag=12'},\n",
       "      {'bitrate': 2176000,\n",
       "       'content_type': 'video/mp4',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/vid/1280x720/aOPOcEVxPItrZ2RR.mp4?tag=12'},\n",
       "      {'content_type': 'application/x-mpegURL',\n",
       "       'url': 'https://video.twimg.com/ext_tw_video/1448215398814560259/pu/pl/4_kPEePepwPbCe8k.m3u8?tag=12&container=fmp4'}]},\n",
       "    'additional_media_info': {'monetizable': False}}]},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': 1448208458604584960,\n",
       " 'in_reply_to_status_id_str': '1448208458604584960',\n",
       " 'in_reply_to_user_id': 14499829,\n",
       " 'in_reply_to_user_id_str': '14499829',\n",
       " 'in_reply_to_screen_name': 'WHO',\n",
       " 'user': {'id': 14499829,\n",
       "  'id_str': '14499829',\n",
       "  'name': 'World Health Organization (WHO)',\n",
       "  'screen_name': 'WHO',\n",
       "  'location': 'Geneva, Switzerland',\n",
       "  'description': 'We are the #UnitedNationsâ€™ health agency - #HealthForAll.\\nâ–¶ï¸ Always check our latest tweets on #COVID19 for updated advice/information.',\n",
       "  'url': 'https://t.co/wVulKuROWG',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/wVulKuROWG',\n",
       "      'expanded_url': 'http://www.who.int',\n",
       "      'display_url': 'who.int',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 9963586,\n",
       "  'friends_count': 1743,\n",
       "  'listed_count': 34215,\n",
       "  'created_at': 'Wed Apr 23 19:56:27 +0000 2008',\n",
       "  'favourites_count': 11879,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': True,\n",
       "  'statuses_count': 64983,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'D0ECF8',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/875476478988886016/_l61qZdR_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/14499829/1610970935',\n",
       "  'profile_link_color': '0396DB',\n",
       "  'profile_sidebar_border_color': '8C8C8C',\n",
       "  'profile_sidebar_fill_color': 'D9D9D9',\n",
       "  'profile_text_color': '000000',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'regular',\n",
       "  'withheld_in_countries': []},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 16,\n",
       " 'favorite_count': 52,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First tweet structure:\")\n",
    "tweets['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'extended_entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three full text tweets:\n",
      "\n",
      "Tweet number 1 :\n",
      "\n",
      "It's International Day for Disaster Risk Reduction\n",
      "\n",
      "#OpenWHO has launched a multi-tiered core curriculum to help equip you with the competencies needed to work within public health emergency response.\n",
      "\n",
      "Start learning today &amp; be #Ready4Response:\n",
      "ðŸ‘‰ https://t.co/hBFFOF0xKL https://t.co/fgZY22RWuS\n",
      "\n",
      "\n",
      "Tweet number 2 :\n",
      "\n",
      "#COVID19 has shown how health emergencies and disasters affect entire communities â€“ especially those with weak health systems, and vulnerable populations like migrants, indigenous peoples, and those living in fragile humanitarian conditions. https://t.co/jpUQpnu0V1\n",
      "\n",
      "\n",
      "Tweet number 3 :\n",
      "\n",
      "It's International Day for Disaster Risk Reduction\n",
      " \n",
      "To better respond to emergencies countries must:\n",
      "âœ… invest in health care systems\n",
      "âœ… achieve gender equity\n",
      "âœ… protect marginalised groups\n",
      "âœ… ensure ready &amp; equitable access to supplies\n",
      " \n",
      "A strong &amp; resilient health system is ðŸ”‘ https://t.co/5NALyjIymp\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First three full text tweets:\\n\")\n",
    "for i in range(3):\n",
    "    print(\"Tweet number\",i+1,\":\\n\")\n",
    "    print(tweets[str(i)][\"full_text\"])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(line):\n",
    "    \"\"\"\n",
    "    Pre-process the tweet text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be pre-processed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = line.lower() ## Transform in lowercase\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    line = tokenizer.tokenize(line)\n",
    "    line = [word for word in line if word not in stop_words]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line = [stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    i = 0\n",
    "    for word in line:\n",
    "        if word[0:4] == 'http':\n",
    "            line = line[:i]\n",
    "        i+=1\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here for every tweet we extract the 'full_text' variable since we will only \n",
    "search inside the text of each tweet. Thus, we pre-process each tweet and \n",
    "store it in a new dictionary called proc_tweets for later use\n",
    "\"\"\"\n",
    "proc_tweets = {}\n",
    "for tweet_id, tweet in zip(tweets.keys(),tweets.values()):\n",
    "    proc_tweets[int(tweet_id)] = process_tweet(tweet['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five processed tweets\n",
      "\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', 'openwho', 'launch', 'multi', 'tier', 'core', 'curriculum', 'help', 'equip', 'compet', 'need', 'work', 'within', 'public', 'health', 'emerg', 'respons', 'start', 'learn', 'today', 'amp', 'ready4respons']\n",
      "\n",
      "\n",
      "['covid19', 'shown', 'health', 'emerg', 'disast', 'affect', 'entir', 'commun', 'especi', 'weak', 'health', 'system', 'vulner', 'popul', 'like', 'migrant', 'indigen', 'peopl', 'live', 'fragil', 'humanitarian', 'condit']\n",
      "\n",
      "\n",
      "['intern', 'day', 'disast', 'risk', 'reduct', 'better', 'respond', 'emerg', 'countri', 'must', 'invest', 'health', 'care', 'system', 'achiev', 'gender', 'equiti', 'protect', 'marginalis', 'group', 'ensur', 'readi', 'amp', 'equit', 'access', 'suppli', 'strong', 'amp', 'resili', 'health', 'system']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"First five processed tweets\\n\")\n",
    "for i in range(3):\n",
    "    print(proc_tweets[i])\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 2:\n",
    "\n",
    "INDEXING:\n",
    "1. Create inverted index\n",
    "2. Make a proposal of 5 queries that will be used to evaluate our search engine\n",
    "3. Apply a TF-IDF ranking to your results\n",
    "\n",
    "EVALUATION:\n",
    "1. Set the ground truth for each document and queryy (binary way)\n",
    "2. Evaluate the algorithm with:\n",
    "    2.1 Precision @K\n",
    "    2.2 Average Precision @K \n",
    "    2.3 Mean Average Precision\n",
    "    2.4 Mean Reciprocal Rank\n",
    "    2.5 Normalizewd Discounted Cummulative Gain\n",
    "3. Choose one vector rpresentation (TF-IDF or word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_info(tweet):\n",
    "    Tweet = tweet['full_text']\n",
    "    Username = tweet['user']['name']\n",
    "    Date = tweet['created_at']\n",
    "    #Hashtags = tweet['entities']['hashtags']['text']\n",
    "\n",
    "    Hashtags = []\n",
    "    hashtags_list = tweet['entities']['hashtags']\n",
    "    for hashtag in hashtags_list:\n",
    "        Hashtags.append(hashtag['text'])\n",
    "\n",
    "    Likes = tweet['favorite_count']\n",
    "    Retweets = tweet['retweet_count']\n",
    "    Url = f\"https://twitter.com/{tweet['user']['screen_name']}/status/{tweet['id_str']}\"\n",
    "    info = [Tweet, Username, Date, Hashtags, Likes, Retweets, Url]\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Generates the index from our database to perform queries from\n",
    "    \n",
    "    Argument:\n",
    "    tweets -- collection of tweets\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    id_index = {}\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)\n",
    "    idf = defaultdict(float)\n",
    "    numDocuments = len(tweets)\n",
    "    for i in range(numDocuments):\n",
    "        tweet = tweets[str(i)]\n",
    "        terms = process_tweet(tweet['full_text']) #get tweet text\n",
    "        info = get_tweet_info(tweet) # get \"document\" info\n",
    "        id_tweet = tweet['id']\n",
    "        id_index[id_tweet]=info\n",
    "        \n",
    "        \n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                index[term].append(id_tweet)  \n",
    "                \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                index[term]= [id_tweet]#'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        norm=0\n",
    "        for term,ids in index.items():\n",
    "            norm += len(ids)**2\n",
    "        norm = math.sqrt(norm)\n",
    "        \n",
    "        for term,ids in index.items():\n",
    "            tf[term].append(np.round(len(ids)/norm,4))\n",
    "            df[term] += 1\n",
    "        \n",
    "        for term in index:\n",
    "            idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "        \n",
    "    return index, tf,df,idf,id_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 122.6 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "index, tf,df,idf,id_index = create_index(tweets)\n",
    "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    query = process_tweet(query)\n",
    "    first = True\n",
    "    docs = {}\n",
    "    for term in query:\n",
    "        try:\n",
    "            list_docs = index[term]\n",
    "            if first:\n",
    "                docs = set(list_docs)\n",
    "                first = False\n",
    "            else:\n",
    "                docs &= set(list_docs)\n",
    "            \n",
    "        except:\n",
    "            break\n",
    "    docs=list(docs)\n",
    "    return docs           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search(query, index):\n",
    "#     query = process_tweet(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "top\n",
      "Top 10 results:\n",
      "\n",
      "[\"RT @DrTedros: I thanked the Prime Minister for ðŸ‡°ðŸ‡¼'s support to #COVAX, but also to various @WHO emergency responses. ðŸ‡°ðŸ‡¼ is one of the top 1â€¦\", 'World Health Organization (WHO)', 'Wed Jul 28 19:07:57 +0000 2021', ['COVAX'], 0, 37, 'https://twitter.com/WHO/status/1420461039926358017']\n",
      "\n",
      "['Dr @Kate_L_OBrien explains what we know so far about  #COVID19 vaccine \"booster\" shots, and why making sure at-risk people ð™šð™«ð™šð™§ð™®ð™¬ð™ð™šð™§ð™š get 1st and 2nd doses should be the top priority. â¬‡ï¸\\n\\n#VaccinEquity https://t.co/igI76vF5tK', 'World Health Organization (WHO)', 'Thu Aug 05 20:40:05 +0000 2021', ['COVID19', 'VaccinEquity'], 574, 279, 'https://twitter.com/WHO/status/1423383330393935873']\n",
      "\n",
      "['@DrTedros \"What happened to you should never happen to anyone. Itâ€™s inexcusable. It is my top priority to ensure that the perpetrators are not excused, but are held to account\"-@DrTedros https://t.co/CbCTLWbiHi', 'World Health Organization (WHO)', 'Tue Sep 28 13:41:01 +0000 2021', [], 93, 23, 'https://twitter.com/WHO/status/1442846811484573699']\n",
      "\n",
      "['@Olympics @DrTedros @Tokyo2020 \"Thatâ€™s why WHOâ€™s top priority is universal health coverage. Our vision is a world in which all people can access the health services they need, where and when they need them, without facing financial hardship\"-@DrTedros #AGoal4All #HealthForAll', 'World Health Organization (WHO)', 'Wed Jul 21 00:47:07 +0000 2021', ['AGoal4All', 'HealthForAll'], 65, 27, 'https://twitter.com/WHO/status/1417647290156716033']\n",
      "\n",
      "['\"As well as looking to accelerate the end of the #COVID19 pandemic, giving children the support they need must be a top priority as we come out of it. We must help them come to terms with what they have experienced and have a chance at a more hopeful future\"-@DrTedros', 'World Health Organization (WHO)', 'Wed Jul 07 13:41:40 +0000 2021', ['COVID19'], 106, 34, 'https://twitter.com/WHO/status/1412768782934806529']\n",
      "\n",
      "['\"Third, change must be driven from the very top, which means ensuring womenâ€™s representation in leadership positions. \\nAt minimum womenâ€™s leadership numbers should be equal with men: 50-50. \\nBut in truth it should be proportional with their share of all jobs: 70-30.\"-@DrTedros', 'World Health Organization (WHO)', 'Thu Jul 01 14:35:26 +0000 2021', [], 44, 12, 'https://twitter.com/WHO/status/1410607987564048385']\n",
      "\n",
      "['@DrTedros \"But because manufacturers have prioritised or been legally obliged to fulfil bilateral deals with rich countries willing to pay top dollar, low-income countries have been deprived of the tools to protect their people\"-@DrTedros #VaccinEquity \\n\\nhttps://t.co/IcyJl2pDLc', 'World Health Organization (WHO)', 'Wed Sep 08 14:41:06 +0000 2021', ['VaccinEquity'], 69, 39, 'https://twitter.com/WHO/status/1435614176018317319']\n",
      "\n",
      "[\"ðŸ‡©ðŸ‡ª is a ðŸ”‘supporter of health emergency preparedness and response &amp; the top donor to the WHO Contingency Fund for Emergencies, contributing more than US$68 million of flexible funding since 2015, supporting WHO's response to more than 120 emergencies.\\n\\nhttps://t.co/o07XThTks0\", 'World Health Organization (WHO)', 'Tue Aug 31 16:58:47 +0000 2021', [], 161, 40, 'https://twitter.com/WHO/status/1432749720359821321']\n",
      "\n",
      "['RT @pahowho: ðŸŒŽðŸ’‰âœˆï¸ðŸš‘ ðŸ“¦ \\nNew Annual Statistical Report on United Nations Procurement\\nâœ… We are among the top 1ï¸âƒ£0ï¸âƒ£  organizations in total purâ€¦', 'World Health Organization (WHO)', 'Wed Jul 07 20:35:32 +0000 2021', [], 0, 16, 'https://twitter.com/WHO/status/1412872937435172878']\n",
      "\n",
      "['@DrTedros \"WHO recognizes that #COVID19 has caused severe disruptions to essential services for immunization, nutrition, noncommunicable diseases, family planning &amp; more. Thatâ€™s why WHOâ€™s top priority is universal health coverage, built on strong #primaryhealthcare\"-@DrTedros #HealthForAll', 'World Health Organization (WHO)', 'Mon Jul 26 06:14:24 +0000 2021', ['COVID19', 'primaryhealthcare', 'HealthForAll'], 46, 12, 'https://twitter.com/WHO/status/1419541592935895058']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query,index)\n",
    "\n",
    "print(\"Top 10 results:\\n\")\n",
    "for d_id in docs[:10]:\n",
    "    print(f\"{id_index[d_id]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "\n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaining elements would became 0 when multiplied to the query_vector\n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    query_vector = [0] * len(terms)\n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    #HINT: use when computing tf for query_vector\n",
    "\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "\n",
    "        ## Compute tf*idf(normalize TF as done with documents)\n",
    "        query_vector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate doc_vectors for matching docs\n",
    "        for doc in index[term]:\n",
    "            # Example of [doc_index, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # Calculate the score of each doc \n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "    # see np.dot\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    #print document titles instead if document id's\n",
    "    #result_docs=[ title_index[x] for x in result_docs ]\n",
    "    if len(result_docs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)\n",
    "    #print ('\\n'.join(result_docs), '\\n')\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    \n",
    "    query = process_tweet(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            list_docs = index[term]\n",
    "            if first:\n",
    "                docs = set(list_docs)\n",
    "                first = False\n",
    "            else:\n",
    "                docs &= set(list_docs)\n",
    "            \n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, id_index)\n",
    "    \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "covid\n",
      "No results found, try again\n",
      "covid\n",
      "No results found, try again\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
    "for d_id in ranked_docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
